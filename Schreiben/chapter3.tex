\chapter{Hauptkomponentenanalyse}   

Die Hauptkomponentenanalyse (engl.\ \emph{Principal Component Analysis}, PCA) ist ein Verfahren zur Dimensionsreduktion von Daten.
Genauer: Es handelt sich um eine Methode, um komplexe Daten auf ihr Wesentliches zu reduzieren, was eine Weiterverarbeitung und Visualisierung erleichtert.

In diesem Kapitel wird zunächst die Intuition hinter der PCA erläutert, bevor der mathematische Hintergrund und insbesondere die Verbindung zur Singulärwertzerlegung beschrieben wird.
Abschließend betrachten wir ein konkretes Anwendungsbeispiel und veranschaulichen dieses mithilfe der Programmiersprache \texttt{Python}.
Die Intuition und Mathematik orientiert sich dabei zum Großteil an~\cite[S.~165-169]{ngMachineLearningCS2292023}.

\section{Intuition der PCA}
Angenommen, beim Familienessen käme die Frage auf, welche der mitgebrachten Weine sich am ähnlichsten sind. 
Um diese Frage zu beantworten, überlegt sich die Familie verschiedene Merkmale und ordnet jedem Wein für jedes Merkmal Zahlen zwischen \num{-3} und \num{3} zu.
Dadurch können die Weine als Punkte im Raum bezüglich der verschiedenen Werte dargestellt und anschließend analysiert werden, welche Weine sich gruppieren, also sich ähneln. 

In \zcref{fig:pcadim}, zu finden im \zcref{appen}, wird dies für verschiedene \(n \coloneqq \emph{Anzahl der Merkmale}\) verdeutlicht. 

Das Problem wird schnell ersichtlich:
Eine visuelle Interpretation ist zwar möglich, allerdings nur im niedrig-dimensionalen Raum, für eine größere Anzahl an Merkmalen (Dimensionen) besteht die Notwendigkeit, die Anzahl zu reduzieren.
Diese Dimensionsreduktion stellt häufig auch unabhängig von der visuellen Interpretation eine sinnvolle Maßnahme dar, falls beispielsweise verschiedene Merkmale stark korrelieren und somit redundant für eine Rekonstruktion der Objekte sind. 

Die Hauptkomponentenanalyse bietet dafür die Möglichkeit, indem neue, unkorrelierte Komponenten konstruiert werden, die sich als Linearkombination aus den bestehenden Merkmalen zusammensetzen.
An unserem konkreten Beispiel könnten beispielsweise Merkmale wie \enquote{Vollmundigkeit} und \enquote{Farbintensität} zu einer neuen Komponente (PC1) zusammengefasst werden.
Anschließend werden die Weine auf den durch PC1 vorgegebenen Unterraum projiziert, was in \zcref{fig:pca2d} veranschaulicht wird.
\begin{figure}[bt]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \caption{}\label{fig:pca2d1}
        \input{plots/pca_projection_1.tex}
        \hspace{20pt}
        \input{plots/pca_projection_2.tex}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \caption{}\label{fig:pca2d2}
        \input{plots/pca_projection_3.tex}
        \hspace{20pt}
        \input{plots/pca_projection_4.tex}
    \end{subfigure}
    \caption{Projektionen im zweidimensionalen Raum}\label{fig:pca2d}
\end{figure}

Der Unterschied zwischen \zcref{fig:pca2d1} und \zcref{fig:pca2d2} zeigt folgendes Problem auf:
Wie kann die neue Komponente optimal gewählt werden, sodass unsere Daten trotz der Dimensionsreduktion so originalgetreu wie möglich rekonstruiert werden können?
Dafür gibt es zwei alternative, aber äquivalente, Formulierungen:
\begin{enumerate}
    \item Die Komponente wird so konstruiert, dass ein Minimum an Informationen verloren wird.
    \item Die Komponente wird so konstruiert, dass eine maximale Varianz erhalten bleibt.
\end{enumerate}
Eine Äquivalenz dieser beiden Aussagen kann durch den Satz des Pythagoras hergeleitet werden, indem das rechtwinklige Dreieck zwischen einem Punkt, dem projizierten Punkt und dem Ursprung betrachtet wird. 
Wird der Informationsverlust (Distanz zwischen der Projektion und dem Original) minimiert, maximiert sich die Varianz (Abstand zum Ursprung).
Es sei angemerkt, dass dafür von einer Zentrierung der Daten ausgegangen wird, also von einem Mittelwert gleich null.

Mit diesem Hintergrund wird intuitiv ersichtlich, dass \zcref{fig:pca2d1} im Vergleich zu \zcref{fig:pca2d2} vorzuziehen ist, was sich im Ergebnis widerspiegelt, in dem die räumliche Verteilung im Wesentlichen erhalten geblieben ist.

Die erste Hauptkomponente (PC1) stellt dann den Einheitsvektor dar, der als Basis den Unterraum aufspannt, auf den die Daten projiziert werden.
In höheren Dimensionen wird die zweite Hauptkomponente unabhängig, also orthogonal zur ersten, gewählt, damit die verbleibende Varianz  erneut maximiert wird.
Dies kann beliebig fortgesetzt werden, die PCA besitzt allerdings die nützliche Eigenschaft, dass die Hauptkomponenten nach \enquote{Wichtigkeit} sortiert sind, also die ersten Komponenten bereits den Großteil der Varianz erklären.
Eine Präzision dieser Aussage wird dabei in der folgenden mathematischen Herleitung gegeben.

\section{Mathematische Herleitung}

Um die vorangegangenen Überlegungen zu formalisieren, wird zunächst die Datenmatrix
\begin{equation*}
    X = 
    \big[
        \begin{matrix}
            x_1 \dots x_d
        \end{matrix}    
    \big] \in \R^{n \times d}
\end{equation*} 
standardisiert, indem wir eine neue Matrix 
\begin{equation*}
    \overline{X} = 
        \begin{bmatrix}
            \overline{x}_1 \dots \overline{x}_d
        \end{bmatrix} \in \R^{n \times d}
\end{equation*} 
definieren mit 
\begin{equation*}
    \overline{x}_{i,j} = \frac{x_{i,j}-\mu_i}{\sigma_i} \quad \text{für } i \in \{1,\ldots,d\} \text{ und } j \in \{1,\ldots,n\},
\end{equation*}  
wobei 
\begin{equation*}
    \mu_i = \frac{1}{n}\sum_{j=1}^{n}x_{i,j}, \quad \sigma_{i}^{2} = \frac{1}{n}\sum_{j=1}^{n}{(x_{i,j} - \mu_{i})}^{2}
\end{equation*}
jeweils die Mittelwerte, bzw.\ die Varianzen der einzelnen Merkmale, also der Spalten sind.
Die Zentrierung um den Ursprung durch die Subtraktion des Mittelwerts vereinfacht dabei spätere Rechnungen erheblich, ist rein formal allerdings nicht zwingend erforderlich.
Durch die Division der Standardabweichung wird Ungenauigkeiten aufgrund verschiedener Skalen der Merkmale vorgebeugt.
Falls Merkmal A beispielsweise das Bruttoinlandsprodukt und Merkmal B die Geburtenrate verschiedener Länder darstellt, wird dadurch eine Vergleichbarkeit gewährleistet.
In den folgenden Berechnungen wird eine Standardisierung angenommen und weiterhin mit \(X\) gearbeitet.
Wir definieren 
\begin{equation*}
    \R^{d} \ni x^{(i)} \coloneqq X_{i,:} \quad \text{für } i \in \{1,\ldots,n\},
\end{equation*}
also die \(i\)-te (transponierte) Zeile von \(X\), wobei jede Zeile je einen Punkt im Raum darstellt.

Das Ziel ist, einen Einheitsvektor \(u \in \R^{d}\) zu finden, sodass nach Projektion der Vektoren \(x^{(i)}\) auf \(u\) eine maximale Varianz, also eine maximale quadratische Abweichung vom Mittelwert, erhalten bleibt.
Die Projektion eines Vektors ist gegeben durch \zcref{rep:proj}.
\begin{repitition}\label{rep:proj}
    Sei \(n \in \N\) und \(u,x \in \R^{n}\) mit \(\norm{u} = 1\).  \\
    Dann ist der orthogonal projizierte Vektor \(\operatorname{proj}_{u}(x)\) von \(x\) auf \(u\) gegeben durch
    \begin{equation*}
        \operatorname{proj}_{u}(x) = \langle x,u \rangle u = (x^{T}u)u.
    \end{equation*}     
\end{repitition}
Um die Varianz zu ermitteln wird zunächst der Mittelwert \(\mu_{\operatorname{proj}}\) der projizierten Vektoren berechnet:
\begin{equation*}
    \mu_{\operatorname{proj}} = \frac{1}{n}\sum_{i=1}^{n}\big(x^{{(i)}^{T}}u\big)u = \left({\left(\frac{1}{n}\sum_{i=1}^{n}x^{(i)}\right)}^{T}u\right)u = \symbf{0},
\end{equation*}
da durch die Standardisierung der Spaltenmittelwert für jede Spalte von \(X\) null beträgt, wodurch
\begin{equation*}
    \sum_{i=1}^{n}x^{(i)} = \symbf{0}.
\end{equation*}
Die Entfernung (Abweichung) vom Mittelwert, also dem Ursprung, für einen beliebigen Vektor \(x^{(i)}\) beträgt
\begin{equation*}
    \norm{\operatorname{proj}_{u}\big(x^{(i)}\big)} = \norm{\big(x^{{(i)}^{T}}u\big)u} =  \abs{\big(x^{{(i)}^{T}}u\big)}\norm{u} = \abs{x^{{(i)}^{T}}u}.
\end{equation*}
Damit ist die Varianz der projizierten Punkte gegeben durch 
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}{\big(x^{{(i)}^{T}}u\big)}^{2} &= \frac{1}{n}\sum_{i=1}^{n}x^{{(i)}^{T}}ux^{{(i)}^{T}}u \\
    &= \frac{1}{n}\sum_{i=1}^{n}u^{T}x^{(i)}x^{{(i)}^{T}}u \qquad (\text{Skalarprodukt kommutativ})\\
    &= u^{T}\left(\frac{1}{n}\sum_{i=1}^{n}x^{(i)}x^{{(i)}^{T}}\right)u
\end{align*}
Definiere 
\begin{equation*}
    \Sigma \coloneqq \frac{1}{n}\sum_{i=1}^{n}x^{(i)}x^{{(i)}^{T}} \in \R^{d \times d}.
\end{equation*}
Diese Matrix ist als \emph{Kovarianzmatrix} bekannt, in unserem Fall ist sie die Kovarianzmatrix der verschiedenen Merkmale.
Es sei angemerkt, dass \(\Sigma\) symmetrisch ist, beachte dafür:
\begin{equation*}
    \Sigma^{T} = {\left(\frac{1}{n}\sum_{i=1}^{n}x^{(i)}x^{{(i)}^{T}}\right)}^{T} = \frac{1}{n}\sum_{i=1}^{n}{\left(x^{(i)}x^{{(i)}^{T}}\right)}^{T} = \Sigma.
\end{equation*}
Damit haben wir unser Ziel auf folgendes Optimierungsproblem reduziert:
\begin{alignat*}{2}
    &\!\max \qquad &&u^{T} \Sigma u, \\
    &\text{u.d.B.}  &&\norm{u}=1.
\end{alignat*}
Dieses Optimierungsproblem wird in der Literatur meist mithilfe von Lagrange-Multiplikatoren gelöst.
In dieser Arbeit werden wir einen anderen Ansatz verfolgen und mit dem, im vorherigen Kapitel bewiesenen, Spektralsatz (\zcref{spec}) vorgehen, wobei sich an~\cite{hsuMachineLearningTheory2016} orientiert wird.

Da \(\Sigma\) symmetrisch ist, kann der Spektralsatz angewendet werden, womit 
\begin{equation*}
    \Sigma = R \Lambda R^{T}
\end{equation*}
für orthogonales \(R =
\big[
    \begin{matrix}
        r_{1}\ldots r_{d}
    \end{matrix}
\big] \in \R^{d \times d}\) und diagonales \(\Lambda = \operatorname{diag}(\lambda_1,\ldots,\lambda_d)\) mit \(\lambda_1 \geq \cdots \geq \lambda_d\).
Für \(w \coloneqq R^{T}u\) gilt dann
\begin{equation*}
    u^{T}\Sigma u = u^{T}R \Lambda R^{T} u = w^{T}\Lambda w = w^{T}
    \begin{bmatrix}
        \lambda_1 w_{1} \\
        \vdots \\
        \lambda_d w_{d}
    \end{bmatrix}
    =\sum_{i=1}^{d}\lambda_{i}w_{i}^{2}.
\end{equation*}
Nach Bedingung gilt \(\norm{u}=1\), womit:
\begin{equation*}
    \norm{w} = \norm{R^{T}u} = \sqrt{\langle R^{T}u,R^{T}u \rangle} = \sqrt{u^{T}RR^{T}u} = \norm{u} = 1.
\end{equation*} 
Da 
\begin{equation*}
    \sum_{i=1}^{d}\lambda_{i}w_{i}^{2} = \lambda_{1}w_{1} + \lambda_{2}w_{2} + \cdots + \lambda_{d}w_{d}
\end{equation*}
und \(\lambda_1 \geq \cdots \geq \lambda_d\) wird der Ausdruck maximiert für \(w = \symbf{e}_{1}\).
Es folgt
\begin{equation*}
    u = Rw = R\symbf{e}_{1} = r_{1},
\end{equation*}
womit \(u\) nach \zcref{cor:spec} gleich dem zugehörigen Eigenvektor zum größten Eigenwert von \(\Sigma\) ist.  